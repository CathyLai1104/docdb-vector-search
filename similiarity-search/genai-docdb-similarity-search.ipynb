{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01cb58e",
   "metadata": {},
   "source": [
    "# Building AI-powered search in Amazon DocumentDB Vector Search using Amazon SageMaker and DocumentDB Vector Search\n",
    "_**Using a pretrained LLM and DocumentDB `Vector Search` for similarity search on product catalog**_\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Language Translation](#Language-Translation)\n",
    "1. [SageMaker Model Hosting](#SageMaker-Model-Hosting)\n",
    "1. [Load data into DocumentDB Vector Search](#DocumentDB Vector Search)\n",
    "1. [Evaluate Search Results](#Evaluate-DocumentDB-vector-Search-Results)\n",
    "\n",
    "## Background\n",
    "\n",
    "In this notebook, we'll build the core components of a textually similar Products. Often people don't know what exactly they are looking for and in that case they just type an item description and hope it will retrieve similar items.\n",
    "\n",
    "One of the core components of searching textually similar items is a fixed length sentence/word embedding i.e. a  “feature vector” that corresponds to that text. The reference word/sentence embedding typically are generated offline and must be stored so they can be efficiently searched. In this use case we are using a pretrained SentenceTransformer model `all-MiniLM-L6-v2` from [HuggingFace Transformers](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).\n",
    "\n",
    "To enable efficient searches for textually similar items, we'll use Amazon SageMaker to generate fixed length sentence embeddings i.e “feature vectors” and use the Nearest Neighbor search in Amazon DocumentDB (with MongoDB compatibility) using the Vector Search. DocumentDB Vector Search lets you store and search for points in vector space and find the \"nearest neighbors\" for those points. Use cases include recommendations (for example, an \"other songs you might like\" feature in a music application), image recognition, and fraud detection.\n",
    "\n",
    "Here are the steps we'll follow to build textually similar items: After some initial setup, we'll host the pretrained language model in SageMaker. Then generate feature vectors for Fashion products from *__feidegger__*, a *__zalandoresearch__* dataset. Those feature vectors will be stored in DocumentDB Vector Search vector datatype. Next, we'll explore some sample text queries, and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7045906",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Install required python libraries for the workshop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb79cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pymongo  tqdm boto3 requests scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3283ad62",
   "metadata": {},
   "source": [
    "### Downloading Zalando Research data\n",
    "\n",
    "The dataset itself consists of 8,732 high-resolution images, each depicting a dress from the available on the Zalando shop against a white-background. Each of the images has five textual annotations orinally in German and are translated to english, each of which has been generated by a separate user. \n",
    "\n",
    "**Downloading Zalando Research data**: Data originally from here: https://github.com/zalandoresearch/feidegger \n",
    "\n",
    " **Citation:** <br>\n",
    " https://github.com/zalandoresearch/feidegger <br>\n",
    " *@inproceedings{lefakis2018feidegger,* <br>\n",
    " *title={FEIDEGGER: A Multi-modal Corpus of Fashion Images and Descriptions in German},* <br>\n",
    " *author={Lefakis, Leonidas and Akbik, Alan and Vollgraf, Roland},* <br>\n",
    " *booktitle = {{LREC} 2018, 11th Language Resources and Evaluation Conference},* <br>\n",
    " *year      = {2018}* <br>\n",
    " *}*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bf338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "filename = 'metadata.json'\n",
    "\n",
    "def download_metadata(url):\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "## The German text has been translated into English and the resulting translation has been stored in this repository for convenience.\n",
    "download_metadata('https://raw.githubusercontent.com/aws-samples/aurora-postgresql-pgvector/main/apgpgvector-similiarity-search/data/FEIDEGGER_release_1.2.json')\n",
    "\n",
    "with open(filename) as json_file:\n",
    "    results = json.load(json_file)\n",
    "\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9cd0d4",
   "metadata": {},
   "source": [
    "## SageMaker Model Hosting\n",
    "\n",
    "In this section will deploy the pretrained `all-MiniLM-L6-v2` Hugging Face SentenceTransformer model into SageMaker and generate 384 dimensional vector embeddings for our product catalog descriptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c409edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20daf917",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "# Hub Model configuration. <https://huggingface.co/models>\n",
    "hub = {\n",
    "  'HF_MODEL_ID': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "  'HF_TASK': 'feature-extraction'\n",
    "}\n",
    "\n",
    "# Deploy Hugging Face Model \n",
    "predictor = HuggingFaceModel(\n",
    "               env=hub, # configuration for loading model from Hub\n",
    "               role=role, # iam role with permissions to create an Endpoint\n",
    "               transformers_version='4.26',\n",
    "               pytorch_version='1.13',\n",
    "               py_version='py39',\n",
    "            ).deploy(\n",
    "               initial_instance_count=1,\n",
    "               instance_type=\"ml.m5.xlarge\",\n",
    "               endpoint_name=\"apg-vector\"\n",
    "            )\n",
    "print(f\"Hugging Face Model has been deployed successfully to SageMaker\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76357427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cls_pooling(model_output):\n",
    "    #First element of model_output contains all token embeddings\n",
    "    return [sublist[0] for sublist in model_output][0]\n",
    "\n",
    "data = {\n",
    "  \"inputs\": ' '.join(results[0].get('descriptions'))\n",
    "}\n",
    "\n",
    "res = cls_pooling( predictor.predict(data=data) )\n",
    "print (len(res))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3dbc0f-61de-4cf2-a6ae-86051ab4b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please re-run the cell if it fails.\n",
    "# Perform a job using realtime inference to generate embeddings ~30 min.\n",
    "\n",
    "def generate_embeddings(data):\n",
    "    r = {}\n",
    "    r['url'] = data['url']\n",
    "    r['descriptions'] = data['descriptions']\n",
    "    r['split'] = data['split']\n",
    "    inp = {'inputs' : ' '.join( data['descriptions'] ) }\n",
    "    vector = cls_pooling( predictor.predict(inp) )\n",
    "    r['descriptions_embeddings'] = vector\n",
    "    return r\n",
    "    \n",
    "workers = 1 * cpu_count()\n",
    "\n",
    "chunksize = 32\n",
    "\n",
    "#Generate Embeddings\n",
    "data = process_map(generate_embeddings, results, max_workers=workers, chunksize=chunksize)\n",
    "\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abbfac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[0].get('descriptions_embeddings'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453e107d",
   "metadata": {},
   "source": [
    "## Amazon DocumentDB \n",
    "\n",
    "vector search for Amazon DocumentDB (with MongoDB compatibility), a new built-in capability that lets you store, index, and search millions of vectors with millisecond response times within your document database.\n",
    "\n",
    "One of the key benefits of using pgvector is that it allows you to perform similarity searches on large datasets quickly and efficiently. This is particularly useful in industries like e-commerce, where businesses need to be able to quickly search through large product catalogs to find the items that best match a customer's preferences. It supports exact and approximate nearest neighbor search, L2 distance, inner product, and cosine distance.\n",
    "\n",
    "To further optimize your searches, you can also use DocumentDB Vector Search's indexing features. By creating indexes on your vector data, you can speed up your searches and reduce the amount of time it takes to find the nearest neighbors to a given vector.\n",
    "\n",
    "In this step we'll get all the translated product descriptions of *__zalandoresearch__* dataset and store those embeddings into DocumentDB Vector Search vector type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a connection to your Amazon DocumentDB (MongoDB compatibility) cluster and creating the database\n",
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient(\n",
    "\"vector-search-recommend.cluster-cku*****st-1.docdb.amazonaws.com:27017\",\n",
    "username=\"masteruser\",\n",
    "password=\"******\",\n",
    "retryWrites=False,\n",
    "tls='true',\n",
    "tlsCAFile='global-bundle.pem')\n",
    "db = client.similarity\n",
    "collection = db.products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468de6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import boto3 \n",
    "import json \n",
    "\n",
    "\n",
    "for x in data:\n",
    "    record = { \"description\": ' '.join(x.get('descriptions', [])),\n",
    "          \"url\": x.get('url'),\n",
    "          \"split\": x.get('split'),\n",
    "          \"descriptions_embeddings\": x.get('descriptions_embeddings')}\n",
    "    #print(record)\n",
    "    rec_id1 = collection.insert_one(record)  \n",
    "\n",
    "collection.create_index ([(\"descriptions_embeddings\",\"vector\")], vectorOptions={\n",
    "\"lists\": 100,\n",
    "\"similarity\": \"euclidean\",\n",
    "\"dimensions\": 384})                                      \n",
    "\n",
    "print (\"Vector embeddings has been successfully loaded into DocumentDB\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a93851",
   "metadata": {},
   "source": [
    "## Evaluate DocumentDB vector Search Results\n",
    "\n",
    "In this step we will use SageMaker realtime inference to generate embeddings for the query and use the embeddings to search the DocumentDB to retrive the nearest neighbours and retrive the relevent product images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea144f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "\n",
    "def similarity_search(search_text):\n",
    "    \n",
    "    data = {\"inputs\": search_text}\n",
    "    res1 = cls_pooling( predictor.predict(data\n",
    "                                          =data) )\n",
    "    \n",
    "    query = {\"vectorSearch\" : {\"vector\" : res1, \"path\": \"descriptions_embeddings\", \"similarity\": \"euclidean\", \"k\": 2}}\n",
    "    projection = {\n",
    "    \"_id\":0,\n",
    "    \"url\":1,\n",
    "    \"description\":1,\n",
    "    \"descriptions_embeddings\": 1}\n",
    "    r = collection.aggregate([{'$search': query},{\"$project\": projection}])\n",
    "    \n",
    "  \n",
    "    urls = []\n",
    "    plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "    for x in r:\n",
    "        url = x[\"url\"].split('?')[0]\n",
    "        urldata = requests.get(url).content\n",
    "        a = io.imread(url)\n",
    "        plt.imshow(a)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8812e7",
   "metadata": {},
   "source": [
    "Using the above function `similarity_search` , lets do some search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2256368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_search(\"red sleeveless summer wear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1d4001-56d4-420e-a9da-43969d7c0a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
